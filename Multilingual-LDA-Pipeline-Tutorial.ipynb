{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual Latent Dirichlet Allocation (LDA) Pipeline - the Tutorial\n",
    "\n",
    "\n",
    "Below is a tutorial on how to process data and to train an LDA on it. First, we're going to use the [library]([multilingual LDA library](https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA) as-is to get an overview of what it can do. Second, we'll redo the same thing but while exposing the underlying pipeline. Third, we're going to dissect the pipeline and inspect the intermediate transformations of the data for you to learn precisely how it works. As an overview, the pipeline looks like that: \n",
    "\n",
    "1. Try to train with words. For this, the comments will need to have words that once [stemmed](https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA/blob/master/Stemming-words-from-multiple-languages.ipynb) and once without stop words will at least have some words that will be present across comments from each other. \n",
    "  1. Forward pass\n",
    "    1. Remove stop words\n",
    "    2. Stem words\n",
    "    3. Vectorize the remaining stemmed words to have their count as features for the LDA. Words will be there as 1-grams, and there will also be some 2-grams (2 words). \n",
    "    4. Learn on LDA on those features\n",
    "  2. Backward pass\n",
    "    1. Inverse the LDA by returning the top features per topic\n",
    "    2. Inverse those top features with the count vectorizer which will get back words (1-grams) or the 2-gram of words. \n",
    "    3. Un-stem the words or the 2-grams with a custom inverse stemming algorithm\n",
    "    4. Stop words won't be reintroduced at this point between 2-grams if there were stop words there normally (TO DO)\n",
    "  3. Finally, split the 1-grams from the 2-grams. Also, extract the top comments for each category.\n",
    "2. If the previous failed, we'll retry with a modified pipeline where we train on n-grams of letters instead of words. To do that, we replace the stemmer by a letter splitter that will split on letters before the featurization. The inverse pass will be hard to recover, but clustering would still work in that case to be able to put each comment in a category, at least, and to find the top comments of those categories, too. \n",
    "\n",
    "Note: The classes imported are clean and have unit tests. Don't hesitate to dive in and to check what's under the hood after or while reading!\n",
    "\n",
    "## Overview: the why\n",
    "\n",
    "We want to get an introspection on the data. We want to automatically categorize comments into categories, find the top comments per category, and to represent the categories by their top words or top n-grams. \n",
    "\n",
    "Let's dive in. First, here is an overview of what the whole thing does. It is only a very simple example designed to be understood easily, so we will ask for two categories here: comments about cats (\"chats\" in French), and dogs (\"chiens\" in French). \n",
    "\n",
    "We have French text here, but the pipeline would work for many languages provided it is supported by the [Snowball stemmer](http://snowball.tartarus.org/texts/stemmersoverview.html) and provided that you have a list of the stop words for the stop words removal part which seems to be quite important after testing without this part.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0.14218066, 0.85781934],\n",
      "       [0.11032926, 0.88967074],\n",
      "       [0.16960699, 0.83039301],\n",
      "       [0.88966976, 0.11033024],\n",
      "       [0.85781743, 0.14218257],\n",
      "       [0.83039307, 0.16960693]])\n",
      "['Un super-chien aboie', 'Les super-chats aiment ronronner']\n",
      "[[('chiens', 3.4911389446318633), ('super', 2.4999405011943825)],\n",
      " [('chats', 3.491141575287711), ('super', 2.5000594988056135)]]\n",
      "[[('super chiens', 2.4921013713235154)], [('super chats', 2.4921054657872785)]]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from artifici_lda.lda_service import train_lda_pipeline_default\n",
    "\n",
    "\n",
    "FR_STOPWORDS = [\n",
    "    \"le\", \"les\", \"la\", \"un\", \"de\", \"en\",  # stop words\n",
    "    \"a\", \"b\", \"c\", \"d\",  # 1 char words are removed too\n",
    "    \"est\", \"sur\", \"tres\", \"donc\", \"sont\",  # can even mix in some more common words / borderline stop words.\n",
    "    # even having slang/texto stop words can be good:\n",
    "    \"ya\", \"pis\", \"yer\"]\n",
    "# Note: this list of stop words is poor and has been crafted for this example.\n",
    "\n",
    "fr_comments = [\n",
    "    \"Un super-chat marche sur le trottoir\",\n",
    "    \"Les super-chats aiment ronronner\",\n",
    "    \"Les chats sont ronrons\",\n",
    "    \"Un super-chien aboie\",\n",
    "    \"Deux super-chiens\",\n",
    "    \"Combien de chiens sont en train d'aboyer?\"\n",
    "]\n",
    "\n",
    "transformed_comments, top_comments, _1_grams, _2_grams = train_lda_pipeline_default(\n",
    "    fr_comments,\n",
    "    n_topics=2,\n",
    "    stopwords=FR_STOPWORDS,\n",
    "    language='french')\n",
    "# More languages: \n",
    "# ['danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', \n",
    "#  'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish', 'turkish']\n",
    "\n",
    "pprint(transformed_comments)\n",
    "pprint(top_comments)\n",
    "pprint(_1_grams)\n",
    "pprint(_2_grams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's in the pipeline\n",
    "\n",
    "Let's dig in the method `train_lda_pipeline_default(...)` and see what it does. In fact, it creates an lda_pipeline using scikit-learn's `Pipeline` class. This class can chain many other classes that we've adapted here to the pipeline and that we've improved for our usage. \n",
    "\n",
    "It effectively chain a `StopWordsRemover()`, a `Stemmer()`, a `CountVectorizer()`, and finally the `LDA()` with their respective (hyper)parameters stored in a dict.\n",
    "\n",
    "Each of those chained data-transforming classes needs to implement those methods: \n",
    "- `fit`: to fit the data before transforming it.\n",
    "- `transform`: to transform the data. \n",
    "- `inverse_transform`: once we have transformed data, we can feed it back into the pipeline in reverse order to get from LDA's topics to a more natural description of those topics. \n",
    "\n",
    "Note that `fit_transform` will be already implemented for each of those classes, which will simply call `fit` and then `transform` right after, on the very-same data. We'll use `fit_transform` everywhere below as a shortcut.\n",
    "\n",
    "So the pipeline basically does this: \n",
    "1. Fit everything and then transform everything, class by class, moving forward in the pipeline. At the output of the LDA, we'll get the top topics per comment. \n",
    "2. We not only want the top topics, but also some description of them. So we need the inverse_transform function to get the words of each topics in a legible manner (e.g.: undo the stemming and undo the featurization).  \n",
    "\n",
    "Let's see how all this can be put together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities of categories for comments:\n",
      "array([[0.14218136, 0.85781864],\n",
      "       [0.11032962, 0.88967038],\n",
      "       [0.16960697, 0.83039303],\n",
      "       [0.88967009, 0.11032991],\n",
      "       [0.85781808, 0.14218192],\n",
      "       [0.83039304, 0.16960696]])\n",
      "Top comments per categories:\n",
      "['Un super-chien aboie', 'Les super-chats aiment ronronner']\n",
      "Top words that defines the categories, and their weighting:\n",
      "[[('chiens', 3.491138564268627),\n",
      "  ('super', 2.4999824170081246),\n",
      "  ('super chiens', 2.4921007611254002)],\n",
      " [('chats', 3.4911393619816162),\n",
      "  ('super', 2.5000175829918696),\n",
      "  ('super chats', 2.492101963052467)]]\n",
      "Same as the top that defines the categories and their weighting, but here the 1-grams are splitted from the 2-grams:\n",
      "[[('chiens', 3.491138564268627), ('super', 2.4999824170081246)],\n",
      " [('chats', 3.4911393619816162), ('super', 2.5000175829918696)]]\n",
      "[[('super chiens', 2.4921007611254002)], [('super chats', 2.492101963052467)]]\n"
     ]
    }
   ],
   "source": [
    "# The code directly below is derived from the file `lda_service/lda_service.py` and is simplified\n",
    "\n",
    "from artifici_lda.data_utils import link_topics_and_weightings, get_top_comments, split_1_grams_from_n_grams, \\\n",
    "    get_lda_params_with_specific_n_cluster_or_language, get_word_weightings\n",
    "from artifici_lda.logic.letter_splitter import LetterSplitter\n",
    "from artifici_lda.logic.stop_words_remover import StopWordsRemover\n",
    "from artifici_lda.logic.stemmer import Stemmer, FRENCH\n",
    "from artifici_lda.logic.lda import LDA\n",
    "from artifici_lda.logic.count_vectorizer import CountVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "LDA_PIPELINE_PARAMS_WORDS = {\n",
    "    'stopwords__stopwords': None,\n",
    "    'stemmer__language': FRENCH,  # ENGLISH\n",
    "    'count_vect__max_df': 0.98,\n",
    "    'count_vect__min_df': 2,\n",
    "    'count_vect__max_features': 10000,\n",
    "    'count_vect__ngram_range': (1, 2),\n",
    "    'count_vect__strip_accents': None,\n",
    "    'lda__n_components': 2,\n",
    "    'lda__max_iter': 750,\n",
    "    'lda__learning_decay': 0.5,\n",
    "    'lda__learning_method': 'online',\n",
    "    'lda__learning_offset': 10,\n",
    "    'lda__batch_size': 25,\n",
    "    'lda__n_jobs': -1,  # Use all CPUs\n",
    "}\n",
    "\n",
    "lda_pipeline = Pipeline([\n",
    "    ('stopwords', StopWordsRemover()),\n",
    "    ('stemmer', Stemmer()),\n",
    "    ('count_vect', CountVectorizer()),\n",
    "    ('lda', LDA()),\n",
    "]).set_params(**LDA_PIPELINE_PARAMS_WORDS)\n",
    "\n",
    "# Fit the data\n",
    "transformed_comments = lda_pipeline.fit_transform(fr_comments)\n",
    "print(\"Probabilities of categories for comments:\")\n",
    "pprint(transformed_comments)\n",
    "\n",
    "top_comments = get_top_comments(fr_comments, transformed_comments)\n",
    "print(\"Top comments per categories:\")\n",
    "pprint(top_comments)\n",
    "\n",
    "# Extract information about data\n",
    "topic_words = lda_pipeline.inverse_transform(X=None)\n",
    "topic_words_weighting = get_word_weightings(lda_pipeline)\n",
    "topics_words_and_weightings = link_topics_and_weightings(topic_words, topic_words_weighting)\n",
    "print(\"Top words that defines the categories, and their weighting:\")\n",
    "pprint(topics_words_and_weightings)\n",
    "\n",
    "# Manipulations on the information for a clean return.\n",
    "_1_grams, _2_grams = split_1_grams_from_n_grams(topics_words_and_weightings)\n",
    "print(\"Same as the top that defines the categories and their weighting, but here the 1-grams are splitted from the 2-grams:\")\n",
    "pprint(_1_grams)\n",
    "pprint(_2_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it works: inspecting each part of the pipeline (forward)\n",
    "\n",
    "Now that we have a good overview, let's dig in and not use the `Pipeline` object to be able to see each intermediate step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from artifici_lda.data_utils import get_params_from_prefix_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original comments:\n",
      "['Un super-chat marche sur le trottoir',\n",
      " 'Les super-chats aiment ronronner',\n",
      " 'Les chats sont ronrons',\n",
      " 'Un super-chien aboie',\n",
      " 'Deux super-chiens',\n",
      " \"Combien de chiens sont en train d'aboyer?\"]\n",
      "\n",
      "Comments without stopwords:\n",
      "['super-chat marche trottoir',\n",
      " 'super-chats aiment ronronner',\n",
      " 'chats ronrons',\n",
      " 'super-chien aboie',\n",
      " 'Deux super-chiens',\n",
      " 'Combien chiens train aboyer?']\n"
     ]
    }
   ],
   "source": [
    "stopwords_params = get_params_from_prefix_dict(\n",
    "    param_prefix=\"stopwords__\", \n",
    "    lda_pipeline_params=LDA_PIPELINE_PARAMS_WORDS)\n",
    "\n",
    "swr = StopWordsRemover(**stopwords_params)\n",
    "\n",
    "print(\"Original comments:\")\n",
    "pprint(fr_comments)\n",
    "comments_without_stopwords = swr.fit_transform(fr_comments)\n",
    "print(\"\")\n",
    "print(\"Comments without stopwords:\")\n",
    "pprint(comments_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original comments (already without stop words):\n",
      "['super-chat marche trottoir',\n",
      " 'super-chats aiment ronronner',\n",
      " 'chats ronrons',\n",
      " 'super-chien aboie',\n",
      " 'Deux super-chiens',\n",
      " 'Combien chiens train aboyer?']\n",
      "\n",
      "Stemmed comments:\n",
      "['sup chat march trottoir',\n",
      " 'sup chat aiment ronron',\n",
      " 'chat ronron',\n",
      " 'sup chien aboi',\n",
      " 'deux sup chien',\n",
      " 'combien chien train aboi']\n",
      "Custom stemmer's cache that was saved for the inverse pass later on which will need to choose the top corresponding words back from their counts:\n",
      "{'aboi': {'aboie': 1, 'aboyer': 1},\n",
      " 'aiment': {'aiment': 1},\n",
      " 'chat': {'chat': 1, 'chats': 2},\n",
      " 'chien': {'chien': 1, 'chiens': 2},\n",
      " 'combien': {'Combien': 1},\n",
      " 'deux': {'Deux': 1},\n",
      " 'march': {'marche': 1},\n",
      " 'ronron': {'ronronner': 1, 'ronrons': 1},\n",
      " 'sup': {'super': 4},\n",
      " 'train': {'train': 1},\n",
      " 'trottoir': {'trottoir': 1}}\n"
     ]
    }
   ],
   "source": [
    "stemmer_params = get_params_from_prefix_dict(\n",
    "    param_prefix=\"stemmer__\", \n",
    "    lda_pipeline_params=LDA_PIPELINE_PARAMS_WORDS)\n",
    "\n",
    "st = Stemmer(**stemmer_params)\n",
    "\n",
    "print(\"Original comments (already without stop words):\")\n",
    "pprint(comments_without_stopwords)\n",
    "comments_without_stopwords_stemmed = st.fit_transform(comments_without_stopwords)\n",
    "print(\"\")\n",
    "print(\"Stemmed comments:\")\n",
    "pprint(comments_without_stopwords_stemmed)\n",
    "print(\"Custom stemmer's cache that was saved for the inverse pass later on which \"\n",
    "      \"will need to choose the top corresponding words back from their counts:\")\n",
    "pprint(st.stemmed_word_to_equiv_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting words to 1-gram and 2-gram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments stemmed and without stopwords:\n",
      "['sup chat march trottoir',\n",
      " 'sup chat aiment ronron',\n",
      " 'chat ronron',\n",
      " 'sup chien aboi',\n",
      " 'deux sup chien',\n",
      " 'combien chien train aboi']\n",
      "\n",
      "Vectorized comments:\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "array([[0, 1, 0, 0, 1, 1, 0],\n",
      "       [0, 1, 0, 1, 1, 1, 0],\n",
      "       [0, 1, 0, 1, 0, 0, 0],\n",
      "       [1, 0, 1, 0, 1, 0, 1],\n",
      "       [0, 0, 1, 0, 1, 0, 1],\n",
      "       [1, 0, 1, 0, 0, 0, 0]], dtype=int64)\n",
      "The features in the matrix contains 1-gram and then 2-grams, such as:\n",
      "['aboi', 'chat', 'chien', 'ronron', 'sup', 'sup chat', 'sup chien']\n"
     ]
    }
   ],
   "source": [
    "count_vect_params = get_params_from_prefix_dict(\n",
    "    param_prefix=\"count_vect__\", \n",
    "    lda_pipeline_params=LDA_PIPELINE_PARAMS_WORDS)\n",
    "\n",
    "cv = CountVectorizer(**count_vect_params)\n",
    "\n",
    "print(\"Comments stemmed and without stopwords:\")\n",
    "pprint(comments_without_stopwords_stemmed)\n",
    "comments_without_stopwords_stemmed_vectorized = cv.fit_transform(comments_without_stopwords_stemmed)\n",
    "print(\"\")\n",
    "print(\"Vectorized comments:\")\n",
    "print(type(comments_without_stopwords_stemmed_vectorized))\n",
    "pprint(comments_without_stopwords_stemmed_vectorized.toarray())\n",
    "print(\"The features in the matrix contains 1-gram and then 2-grams, such as:\")\n",
    "pprint(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA on the word features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original comments:\n",
      "['Un super-chat marche sur le trottoir',\n",
      " 'Les super-chats aiment ronronner',\n",
      " 'Les chats sont ronrons',\n",
      " 'Un super-chien aboie',\n",
      " 'Deux super-chiens',\n",
      " \"Combien de chiens sont en train d'aboyer?\"]\n",
      "Comments, featurized:\n",
      "array([[0, 1, 0, 0, 1, 1, 0],\n",
      "       [0, 1, 0, 1, 1, 1, 0],\n",
      "       [0, 1, 0, 1, 0, 0, 0],\n",
      "       [1, 0, 1, 0, 1, 0, 1],\n",
      "       [0, 0, 1, 0, 1, 0, 1],\n",
      "       [1, 0, 1, 0, 0, 0, 0]], dtype=int64)\n",
      "\n",
      "Clusterized comments:\n",
      "array([[0.14218173, 0.85781827],\n",
      "       [0.11032981, 0.88967019],\n",
      "       [0.16960697, 0.83039303],\n",
      "       [0.88967027, 0.11032973],\n",
      "       [0.85781842, 0.14218158],\n",
      "       [0.83039303, 0.16960697]])\n",
      "Let's see their category (argmax on inner dimension):\n",
      "array([1, 1, 1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "lda_params = get_params_from_prefix_dict(\n",
    "    param_prefix=\"lda__\", \n",
    "    lda_pipeline_params=LDA_PIPELINE_PARAMS_WORDS)\n",
    "\n",
    "lda = LDA(**lda_params)\n",
    "\n",
    "print(\"Original comments:\")\n",
    "pprint(fr_comments)\n",
    "print(\"Comments, featurized:\")\n",
    "pprint(comments_without_stopwords_stemmed_vectorized.toarray())\n",
    "print(\"\")\n",
    "comments_lda = lda.fit_transform(comments_without_stopwords_stemmed_vectorized)\n",
    "print(\"Clusterized comments:\")\n",
    "pprint(comments_lda)\n",
    "print(\"Let's see their category (argmax on inner dimension):\")\n",
    "pprint(comments_lda.argmax(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting each part of the pipeline (backwards)\n",
    "\n",
    "### Inverse of the LDA gives us topics' features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([2, 4, 6]), array([1, 4, 5])]\n"
     ]
    }
   ],
   "source": [
    "a = lda.inverse_transform(None)  # None here for getting the fitted categories. \n",
    "pprint(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse of the CountVectorizer gives us the words from features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['chien', 'sup', 'sup chien'], ['chat', 'sup', 'sup chat']]\n"
     ]
    }
   ],
   "source": [
    "b = cv.inverse_transform(a)\n",
    "pprint(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Stemming here yields the most common original word for it's stemmed version\n",
    "\n",
    "[More info on how the Inverse Stemming here](https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA/blob/master/Stemming-words-from-multiple-languages.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['chiens', 'super', 'super chiens'], ['chats', 'super', 'super chats']]\n"
     ]
    }
   ],
   "source": [
    "c = st.inverse_transform(b)\n",
    "pprint(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse stop words removal here does nothing\n",
    "\n",
    "The function basically returns its argument. This is a point that could be improved with a custom algorithm, such as the Stemmer's inverse pass which is custom here. For example, it would be possible to scan back each comment and to find occurences with a regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['chiens', 'super', 'super chiens'], ['chats', 'super', 'super chats']]\n"
     ]
    }
   ],
   "source": [
    "d = swr.inverse_transform(c)\n",
    "pprint(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You now have a quite precise overview on how does this [multilingual LDA](https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA) works. \n",
    "\n",
    "On one hand, it's reasy to use and it's quite straightforward. \n",
    "\n",
    "On the other hand, each class in the pipeline has its own behavior. Here we inherit from some Scikit-learn classes and add them a few extras (such as most of the backward passes), and we also add of our own classes (such as the Stemmer class where [Snowball](https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA/blob/master/Stemming-words-from-multiple-languages.ipynb) is used for the forward pass). It would be easy to change the implementation of the LDA by creating another class, or to use other algorithms.\n",
    "\n",
    "For more information, don't hesitate to dive into the code. There also are unit tests. \n",
    "\n",
    "### License\n",
    "\n",
    "This [project](https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA) is published under the [MIT License (MIT)](https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA/blob/master/LICENSE).\n",
    "\n",
    "Copyright (c) [2018 Artifici online services inc](https://github.com/ArtificiAI).\n",
    "\n",
    "Coded by [Guillaume Chevalier](https://github.com/guillaume-chevalier) at [Neuraxio Inc.](https://github.com/Neuraxio)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
