{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Some code is inspired from: \n",
    "# https://github.com/scikit-learn/scikit-learn/blob/master/examples/applications/plot_topics_extraction_with_nmf_lda.py\n",
    "# which is available under the following license: BSD 3-Clause\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import Stemmer\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "language = 'french'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ceci est un certain texte,  |  cec est un certain text ,\n",
      "Ceci est un autre texte.  |  cec est un autr text .\n",
      "Et en voici un troixième.  |  et en voic un troixiem .\n",
      "Et un quatrième.  |  et un quatriem .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample_dataset = [\n",
    "    \"Ceci est un certain texte,\",\n",
    "    \"Ceci est un autre texte.\",\n",
    "    \"Et en voici un troixième.\",\n",
    "    \"Et un quatrième.\",\n",
    "]\n",
    "\n",
    "\n",
    "def lemmatize(document, language):\n",
    "    # Split on spaces and convert words to their \n",
    "    # lowercase lemmas, whilst ignoring punctuation.\n",
    "    document = document.lower()\n",
    "    for punctuation_character in punctuation:\n",
    "        document = document.replace(\n",
    "            punctuation_character, \" {} \".format(punctuation_character)\n",
    "        )\n",
    "    document = document.replace(\"  \", \" \").replace(\"  \", \" \").strip()\n",
    "    \n",
    "    words_or_punct = document.split(\" \")\n",
    "    \n",
    "    stemmer = Stemmer.Stemmer(language)\n",
    "    stemmed_words = stemmer.stemWords(words_or_punct)\n",
    "    \n",
    "    lemmatized_document = \" \".join(stemmed_words)\n",
    "    return lemmatized_document\n",
    "\n",
    "\n",
    "lemmatized_dataset = [lemmatize(doc, language) for doc in sample_dataset]\n",
    "\n",
    "# Result of lemmatization:\n",
    "for i in range(len(sample_dataset)):\n",
    "    print(sample_dataset[i], \" | \", lemmatized_dataset[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for LDA are Term Frequencies (TF) of the lemmatized dataset...\n",
      "Fiting the LDA on the data...\n",
      "Topics in the LDA model:\n",
      "topic #0: 'et' 'cec est un' 'cec'\n",
      "topic #1: 'est' 'text' 'cec est'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_vocab_size = 10000\n",
    "n_topics = 2\n",
    "ngram_range = (1, 3)  # 1-gram to 3-grams will be used.\n",
    "\n",
    "\n",
    "print(\"Features for LDA are Term Frequencies (TF) of the lemmatized dataset...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=max_vocab_size, \n",
    "                                ngram_range=ngram_range)\n",
    "tf = tf_vectorizer.fit_transform(lemmatized_dataset)\n",
    "\n",
    "print(\"Fiting the LDA on the data...\")\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)\n",
    "\n",
    "\n",
    "print(\"Topics in the LDA model:\")\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for i, topic in enumerate(model.components_):\n",
    "        top_topics = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        escape = lambda x: \"'\" + x + \"'\"\n",
    "        print(\"topic #{}:\".format(i), \" \".join([escape(feature_names[i]) for i in top_topics]))\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.05844558, 0.94007403, 1.06608669, 0.82795306, 0.9037063 ,\n",
       "        1.38864166, 0.9684819 ],\n",
       "       [1.35535886, 1.39141826, 1.37174657, 1.48673973, 1.34355707,\n",
       "        0.74135258, 1.40593732]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cec', 'cec est', 'cec est un', 'est', 'est un', 'et', 'text']\n"
     ]
    }
   ],
   "source": [
    "print(tf_feature_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
