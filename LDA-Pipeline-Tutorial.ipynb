{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA) Pipeline Example in Python\n",
    "\n",
    "Below is a tutorial on how to process data and to train an LDA on it. \n",
    "\n",
    "Knowing that the LDA perform better with stop words removal and with lemmatized words, it's results are often ugly on the lemmatized words. To fix that, it's a good thing to be able to do an inverse-lemmatization on the topic words (or topic n-grams) yield by the LDA once trained on the data. \n",
    "\n",
    "So here we are: let's do a pipeline that looks like that: \n",
    "\n",
    "1. Load a dataset of many comments (or documents)\n",
    "2. Transform comments to remove stop words\n",
    "3. Lemmatize the comments without stop words for a better LDA\n",
    "4. Perform LDA topic modeling\n",
    "5. Recover words from inverse (backwards) lemmatization on topic words \n",
    "6. Clean topic words are available\n",
    "\n",
    "Note: The classes imported are clean and have unit tests. Don't hesitate to dive in and to check what's under the hood!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from app.data.load_sample_data import load_sample_data\n",
    "from app.logic.stop_words_remover import StopWordsRemover\n",
    "from app.logic.stemmer import Stemmer\n",
    "from app.logic.lda import LDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a dataset of many comments (or documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages, comments = load_sample_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform comments to remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_en_stopwords = StopWordsRemover()\n",
    "comments_without_stopwords = fr_en_stopwords.remove_from_many_strings(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize the comments without stop words for clean texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_stemmer = Stemmer(language='french')\n",
    "stemmed_comments = [french_stemmer.lemmatize(thread) for thread in comments_without_stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform LDA topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(n_topics=2, max_iter=5, learning_method='online', learning_offset=50.)\n",
    "lda_sklearn, feature_names = lda.fit(stemmed_comments[-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recover words from inverse (backwards) lemmatization on topic words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (n_components=10, doc_topic_prior=None, topic_word_prior=None, learning_method=None, learning_decay=0.7, \n",
    "# learning_offset=10.0, max_iter=10, batch_size=128, evaluate_every=-1, total_samples=1000000.0, perp_tol=0.1, \n",
    "# mean_change_tol=0.001, max_doc_update_iter=100, n_jobs=1, verbose=0, random_state=None, n_topics=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some clean topic words (or expressions) are then available!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: clean below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('stopwords', StopWordsRemover()),\n",
    "                     ('stemmer', Stemmer()),\n",
    "                     ('lda', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 2, perplexity: 49.1109\n",
      "iteration: 2 of max_iter: 2, perplexity: 44.6179\n",
      "2 -292.4564070040818 44.61790143210732\n",
      "iteration: 1 of max_iter: 3, perplexity: 49.1109\n",
      "iteration: 2 of max_iter: 3, perplexity: 44.6179\n",
      "iteration: 3 of max_iter: 3, perplexity: 42.1824\n",
      "3 -288.13421703247997 42.18238929273422\n",
      "iteration: 1 of max_iter: 4, perplexity: 49.1109\n",
      "iteration: 2 of max_iter: 4, perplexity: 44.6179\n",
      "iteration: 3 of max_iter: 4, perplexity: 42.1824\n",
      "iteration: 4 of max_iter: 4, perplexity: 40.6754\n",
      "4 -285.33295079087077 40.6753695112215\n",
      "iteration: 1 of max_iter: 5, perplexity: 49.1109\n",
      "iteration: 2 of max_iter: 5, perplexity: 44.6179\n",
      "iteration: 3 of max_iter: 5, perplexity: 42.1824\n",
      "iteration: 4 of max_iter: 5, perplexity: 40.6754\n",
      "iteration: 5 of max_iter: 5, perplexity: 39.6464\n",
      "5 -283.3600458998749 39.64641770588519\n",
      "iteration: 1 of max_iter: 6, perplexity: 49.1109\n",
      "iteration: 2 of max_iter: 6, perplexity: 44.6179\n",
      "iteration: 3 of max_iter: 6, perplexity: 42.1824\n",
      "iteration: 4 of max_iter: 6, perplexity: 40.6754\n",
      "iteration: 5 of max_iter: 6, perplexity: 39.6464\n",
      "iteration: 6 of max_iter: 6, perplexity: 38.8972\n",
      "6 -281.89107993697525 38.897232964041386\n",
      "iteration: 1 of max_iter: 7, perplexity: 49.1109\n",
      "iteration: 2 of max_iter: 7, perplexity: 44.6179\n",
      "iteration: 3 of max_iter: 7, perplexity: 42.1824\n",
      "iteration: 4 of max_iter: 7, perplexity: 40.6754\n",
      "iteration: 5 of max_iter: 7, perplexity: 39.6464\n",
      "iteration: 6 of max_iter: 7, perplexity: 38.8972\n",
      "iteration: 7 of max_iter: 7, perplexity: 38.3265\n",
      "7 -280.7528410496857 38.32647048545758\n",
      "iteration: 1 of max_iter: 8, perplexity: 49.1109\n",
      "iteration: 2 of max_iter: 8, perplexity: 44.6179\n",
      "iteration: 3 of max_iter: 8, perplexity: 42.1824\n",
      "iteration: 4 of max_iter: 8, perplexity: 40.6754\n",
      "iteration: 5 of max_iter: 8, perplexity: 39.6464\n",
      "iteration: 6 of max_iter: 8, perplexity: 38.8972\n",
      "iteration: 7 of max_iter: 8, perplexity: 38.3265\n",
      "iteration: 8 of max_iter: 8, perplexity: 37.8769\n",
      "8 -279.8443315888072 37.87692049512021\n",
      "iteration: 1 of max_iter: 9, perplexity: 49.1109\n",
      "iteration: 2 of max_iter: 9, perplexity: 44.6179\n",
      "iteration: 3 of max_iter: 9, perplexity: 42.1824\n",
      "iteration: 4 of max_iter: 9, perplexity: 40.6754\n",
      "iteration: 5 of max_iter: 9, perplexity: 39.6464\n",
      "iteration: 6 of max_iter: 9, perplexity: 38.8972\n",
      "iteration: 7 of max_iter: 9, perplexity: 38.3265\n",
      "iteration: 8 of max_iter: 9, perplexity: 37.8769\n",
      "iteration: 9 of max_iter: 9, perplexity: 37.5136\n",
      "9 -279.10222990092257 37.51362815517507\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "for i in range(2, 10):\n",
    "    \n",
    "    # lda = LDA(n_topics=2, max_iter=i, learning_method='online', learning_offset=50.)\n",
    "    \n",
    "    lda = LDA(\n",
    "        n_topics=2, max_iter=i, learning_decay=0.7, learning_method='online', \n",
    "        learning_offset=10.0, batch_size=128, total_samples=1000000.0, \n",
    "        mean_change_tol=0.001, max_doc_update_iter=100, \n",
    "        verbose=1,\n",
    "        n_jobs=-1,  # Use all CPUs\n",
    "        evaluate_every=1)\n",
    "    \n",
    "    lda_sklearn, feature_names = lda.fit(stemmed_comments[-2])\n",
    "    print(i, lda.score(stemmed_comments[-2]), lda.perplexity(stemmed_comments[-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 5\n",
      "iteration: 1 of max_iter: 100, perplexity: 43.5576\n",
      "iteration: 2 of max_iter: 100, perplexity: 40.0195\n",
      "iteration: 3 of max_iter: 100, perplexity: 38.2824\n",
      "iteration: 4 of max_iter: 100, perplexity: 37.3002\n",
      "iteration: 5 of max_iter: 100, perplexity: 36.6836\n",
      "iteration: 6 of max_iter: 100, perplexity: 36.2304\n",
      "iteration: 7 of max_iter: 100, perplexity: 35.8177\n",
      "iteration: 8 of max_iter: 100, perplexity: 35.3896\n",
      "iteration: 9 of max_iter: 100, perplexity: 34.9447\n",
      "iteration: 10 of max_iter: 100, perplexity: 34.5067\n",
      "iteration: 11 of max_iter: 100, perplexity: 34.1005\n",
      "iteration: 12 of max_iter: 100, perplexity: 33.7404\n",
      "iteration: 13 of max_iter: 100, perplexity: 33.4277\n",
      "iteration: 14 of max_iter: 100, perplexity: 33.1558\n",
      "iteration: 15 of max_iter: 100, perplexity: 32.9200\n",
      "iteration: 16 of max_iter: 100, perplexity: 32.7227\n",
      "iteration: 17 of max_iter: 100, perplexity: 32.5636\n",
      "iteration: 18 of max_iter: 100, perplexity: 32.4344\n",
      "iteration: 19 of max_iter: 100, perplexity: 32.3269\n",
      "iteration: 20 of max_iter: 100, perplexity: 32.2366\n",
      "-218.8054356943098 32.2365903566732\n",
      "-53.374855110614405 207.98906601030046\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train, test = sklearn.model_selection.train_test_split(stemmed_comments[-2], test_size=0.2)\n",
    "\n",
    "print(len(train), len(test))\n",
    "\n",
    "\n",
    "lda = LDA(\n",
    "    n_topics=2, max_iter=100, learning_decay=0.5, learning_method='online', \n",
    "    learning_offset=10.0, batch_size=128,\n",
    "    mean_change_tol=0.001, max_doc_update_iter=100, \n",
    "    verbose=1,\n",
    "    n_jobs=-1,  # Use all CPUs\n",
    "    evaluate_every=1)\n",
    "\n",
    "lda_sklearn, feature_names = lda.fit(train)\n",
    "\n",
    "print(lda.score(train), lda.perplexity(train))\n",
    "print(lda.score(test), lda.perplexity(test))\n",
    "# -182.51029019702543   27.615270885701467\n",
    "# -52.05494670051718    41.19061662552155   \n",
    "# -41.973876576664225  189.9450029464927    \n",
    "# -32.432059761463556 3320.9791408284505    \n",
    "# perplexity should go down and score should go down too (more negative). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stemmed_comments[-2]), len(stemmed_comments[-3]), len(stemmed_comments[-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "([(len(i), a) for a, i in enumerate(stemmed_comments)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fr_en_stopwords = StopWordsRemover()\n",
    "fr_en_stopwords.remove_from_string(\"Le chat s'est assis sur le tapis aujour'hui!!! il est très comfortable et n'est pas déçu, tout ronron!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
